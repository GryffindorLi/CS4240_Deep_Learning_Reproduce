{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies\n"
      ],
      "metadata": {
        "id": "tyhaTdd9ZF1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyuTzmaYtW9F"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/GryffindorLi/CS4240_Deep_Learning_Reproduce.git\n",
        "! pip install -r CS4240_Deep_Learning_Reproduce/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the tokenizer\n"
      ],
      "metadata": {
        "id": "qnsIVdw0ZJ9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JefvkKMU9KDZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertTokenizer, AlbertModel\n",
        "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "# model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
        "text = \"fairness harm\"\n",
        "max_length = -3\n",
        "ll = {\"a\":[\"equity\", \"amoral\", \"purity\", \"degradation\", \"loyalty\", \n",
        "              \"care\", \"cheating\", \"betrayal\", \"sabotage\", \"authority\", \"harm\"]}\n",
        "for i in ll:\n",
        "  for j in ll[i]:\n",
        "    encoded_input = tokenizer.convert_ids_to_tokens(tokenizer.encode(j))\n",
        "    \n",
        "    # if max_length < len(encoded_input)-2 :\n",
        "    print(encoded_input)\n",
        "    # max_length = len(encoded_input)-2\n",
        "max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the code"
      ],
      "metadata": {
        "id": "AHhsDWoXZRkA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "onr_gNF_YdkO"
      },
      "outputs": [],
      "source": [
        "! rm -rf CS4240_Deep_Learning_Reproduce/output\n",
        "!python CS4240_Deep_Learning_Reproduce/cli.py \\\n",
        "  --method pet --pattern_ids 0 --data_dir CS4240_Deep_Learning_Reproduce/data \\\n",
        "  --model_type albert --model_name_or_path albert-base-v2 --task_name mftc --output_dir CS4240_Deep_Learning_Reproduce/output \\\n",
        "  --do_train --do_eval --pet_per_gpu_eval_batch_size 1  --pet_num_train_epochs 3 --pet_num_train_epochs 3 \\\n",
        "  --train_examples 100 --test_examples 50 --unlabeled_examples 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfYKjIliHtza"
      },
      "outputs": [],
      "source": [
        "!python cli.py --method pet --pattern_ids 1 --data_dir data_sample --model_type albert --model_name_or_path albert-base-v2 --task_name mftc --output_dir output --do_train --do_eval --pet_per_gpu_eval_batch_size 1 --train_examples 100 --test_examples 50 --unlabeled_examples 150"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run bert tramsformer"
      ],
      "metadata": {
        "id": "aDFdSZYyZgUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SentencePiece\n",
        "!pip install simpletransformers\n",
        "import pandas as pd\n",
        "import sklearn.metrics as skm\n",
        "from google.colab import files\n",
        "import os\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
      ],
      "metadata": {
        "id": "ihYYBU7YZw2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"data/labeled.csv\")\n",
        "cols = df.columns[2:]\n",
        "from itertools import combinations\n",
        "combs = combinations(cols, 2)\n",
        "labels = []\n",
        "for comb in combs:\n",
        "  string = \"\"\n",
        "  if len(comb) > 1:\n",
        "      string = comb[0] + \" \" + comb[1]\n",
        "  else:\n",
        "      string = comb[0]\n",
        "  labels.append(string)\n",
        "\n",
        "for i in cols: \n",
        "  labels.append(i)\n",
        "\n",
        "print(labels)\n",
        "\n",
        "def data_loader_dl(datapath):\n",
        "  df = pd.read_csv(datapath)\n",
        "  arr = []\n",
        "  cols = df.columns\n",
        "  # cols = df.iloc[:,2:]\n",
        "  for index,row in df.iterrows():\n",
        "    label = \"\"\n",
        "    count = 0\n",
        "    for index in range(len(row)):\n",
        "      if row[index] == 1:\n",
        "        label +=  cols[index] + \" \"\n",
        "        count += 1\n",
        "      if count == 2:\n",
        "        break\n",
        "    if label != \"\" :\n",
        "      # print(label)\n",
        "      arr.append(label[:-1])\n",
        "  df[\"labels\"] = arr\n",
        "  return df[['text','labels']]\n",
        "\n",
        "\n",
        "train_set = data_loader_dl(\"data/labeled.csv\")\n",
        "test_set = data_loader_dl(\"data/test.csv\")"
      ],
      "metadata": {
        "id": "tpiKh-2VZ57g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete outputs/ directory\n",
        "! rm -rf outputs\\\n",
        "\n",
        "# specify hyperparameters and model arguments\n",
        "model_args = ClassificationArgs(output_dir= \"outputs/untrained_model\")\n",
        "model_args.reprocess_input_data = True\n",
        "model_args.overwrite_output_dir = True\n",
        "model_args.manual_seed = 42\n",
        "model_args.use_multiprocessing = True\n",
        "model_args.train_batch_size = 32\n",
        "model_args.eval_batch_size = 32\n",
        "model_args.labels_list = labels\n",
        "\n",
        "# init bert\n",
        "model = ClassificationModel('bert', 'bert-base-cased', num_labels=len(labels), args=model_args, use_cuda=True)\n",
        "# Evaluate the untrained model on test set\n",
        "result, _ , wrong_predictions = model.eval_model(test_set)\n",
        "\n",
        "for i in [3]:\n",
        "  model_args.num_train_epochs = i\n",
        "  model_args.output_dir = \"outputs/epochs_\"+str(i)\n",
        "\n",
        "  # Create a TransformerModel\n",
        "  model = ClassificationModel('bert', 'bert-base-cased', num_labels=len(labels), args=model_args, use_cuda=True)\n",
        "\n",
        "  # Train the model\n",
        "  model.train_model(train_set, show_running_loss=True)\n",
        "\n",
        "  # Evaluate the model on test set\n",
        "  result, _ , wrong_predictions = model.eval_model(test_set)\n",
        "  source = '/content/outputs/epochs_'+str(i)+'/eval_results.txt'\n",
        "  dst = '/content/outputs/epochs_'+str(i)+'/eval_results_epochs_'+str(i)+'.txt'\n",
        "  os.rename(source,dst)\n",
        "  files.download(dst)"
      ],
      "metadata": {
        "id": "BBK60_KPZ6gT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PET project",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}